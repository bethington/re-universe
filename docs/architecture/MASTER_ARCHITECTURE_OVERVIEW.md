# D2Docs Master Architecture Overview
## AI-Orchestrated Live Analysis Platform

---

## ğŸ¯ **System Overview**

D2Docs is an intelligent reverse engineering platform that combines static analysis, live dynamic analysis, community knowledge mining, and AI-powered insights to create a comprehensive understanding of Diablo 2's codebase. The platform leverages proven Ghidra-MCP workflows with multi-model AI orchestration to provide cost-effective, high-quality analysis.

## ğŸ—ï¸ **Core Architecture Principles**

### **1. Hierarchical Knowledge Organization**
```
ğŸ® Diablo 2 Systems (Game-Level Architecture)
â”œâ”€â”€ ğŸ‘¤ Player System
â”‚   â”œâ”€â”€ ğŸ“Š Character Progression Subsystem
â”‚   â”‚   â”œâ”€â”€ ğŸ“ˆ Experience/Leveling Module (D2Game.dll)
â”‚   â”‚   â”œâ”€â”€ ğŸŒŸ Skill Tree Module (D2Game.dll)
â”‚   â”‚   â””â”€â”€ ğŸ“‹ Stat Calculation Module (D2Common.dll)
â”‚   â””â”€â”€ ğŸ’ Inventory Management Subsystem
â”œâ”€â”€ âš”ï¸ Combat System
â”œâ”€â”€ ğŸŒ World/Level System
â””â”€â”€ ğŸ”— Network/Multiplayer System
```

### **2. Multi-Source Knowledge Integration**
- **Static Analysis**: Ghidra-MCP with 110 tools and proven documentation workflows
- **Community Knowledge**: Automated mining from GitHub, forums, and technical sources
- **Live Analysis**: Windows container with native Detours hooks
- **AI Synthesis**: Multi-model orchestration for intelligent analysis and documentation

### **3. Cost-Optimized AI Orchestration**
- **Claude Opus 4.6** (20%): Master coordinator, quality control, strategic decisions
- **Claude Sonnet** (30%): Heavy analysis, complex documentation, system discovery
- **Claude Haiku** (50%): Batch processing, template generation, routine tasks

## ğŸ—„ï¸ **Data Architecture**

### **Primary Data Stores**
```
ğŸ“Š PostgreSQL + pgvector (Unified Knowledge Store)
â”œâ”€â”€ BSim Database (Existing)
â”‚   â”œâ”€â”€ exetable - Executable metadata
â”‚   â”œâ”€â”€ desctable - Function descriptions
â”‚   â””â”€â”€ vectable - Vector signatures
â”œâ”€â”€ Hierarchical Knowledge (New)
â”‚   â”œâ”€â”€ d2_systems - Game system architecture
â”‚   â”œâ”€â”€ d2_subsystems - Subsystem organization
â”‚   â”œâ”€â”€ d2_modules - Module-level functionality
â”‚   â””â”€â”€ community_function_knowledge - Community insights
â”œâ”€â”€ Vector Search (New)
â”‚   â”œâ”€â”€ Function embeddings (1536 dimensions)
â”‚   â”œâ”€â”€ System/subsystem embeddings
â”‚   â””â”€â”€ Community knowledge embeddings
â””â”€â”€ Provenance & Trust (New)
    â”œâ”€â”€ community_sources - Source attribution
    â”œâ”€â”€ system_interactions - Cross-system relationships
    â””â”€â”€ trust scoring and validation metrics
```

## ğŸ¤– **AI Orchestration Layer**

### **Master Coordinator (Opus 4.6)**
```python
class AIOrchestrator:
    """
    Master coordinator responsible for:
    - Workflow planning and task assignment
    - Quality control and validation
    - Cost optimization and budget management
    - Performance monitoring and optimization
    """

    def route_request(self, query, context):
        complexity = self.analyze_complexity(query)
        if complexity.requires_deep_analysis:
            return self.assign_to_sonnet(query, context)
        elif complexity.is_batch_operation:
            return self.assign_to_haiku(query, context)
        else:
            return self.handle_directly(query, context)
```

### **Analysis Workers (Sonnet)**
- Execute FUNCTION_DOC_WORKFLOW_V4.md templates
- Perform complex system architecture discovery
- Generate comprehensive documentation using proven formats
- Cross-validate community knowledge against binary analysis

### **Batch Processors (Haiku)**
- Mass variable renaming with Hungarian notation
- Template-based script generation with caching
- Community data extraction and entity recognition
- Progress tracking and completion reporting

## ğŸŒ **User Interface Architecture**

### **Preserved Three-Panel Flow**
```
Version Selection â†’ Binary Selection â†’ Function List â†’ Enhanced Detail View
     Panel 1            Panel 2         Panel 3         Multi-Tab Interface
```

### **Enhanced Detail View Tabs**
1. **Comparison Tab** (Existing) - Cross-version function comparison
2. **Assembly Tab** (Enhanced) - Static analysis + live correlation
3. **AI Documentation Tab** (New) - Comprehensive AI-generated documentation
4. **Community Knowledge Tab** (New) - Community insights and sources

### **AI Chat Interface**
- **Global Floating Assistant**: Always accessible, context-aware
- **Knowledge Explorer Tab**: Advanced interface with hierarchy browser
- **Contextual Chat**: Function-specific assistance with suggestions

## ğŸ•·ï¸ **Community Knowledge Mining**

### **Automated Discovery Pipeline**
```mermaid
graph TD
    A[Cron Scheduler] --> B[GitHub API Scanner]
    A --> C[Web Content Scanner]
    B --> D[Repository Analysis]
    C --> E[Content Extraction]
    D --> F[Function Prototype Discovery]
    E --> F
    F --> G[Trust Score Calculation]
    G --> H[Validation Engine]
    H --> I[Knowledge Storage]
    I --> J[Vector Embedding]
    J --> K[Integration with BSim]
```

### **Trust Scoring Algorithm**
```python
def calculate_trust_score(source):
    """
    Multi-factor trust scoring:
    - Repository quality (stars, commits, age)
    - Author reputation and contribution history
    - Content quality indicators (documentation, tests)
    - Historical accuracy of previous contributions
    - Cross-validation against binary analysis
    """
    base_score = 0.5
    # Repository factors (max +0.35)
    # Author factors (max +0.1)
    # Content quality (max +0.15)
    # Historical accuracy (max +0.2)
    return min(total_score, 1.0)
```

## ğŸ”„ **Live Analysis Integration**

### **Windows Container Architecture**
```yaml
live-analyzer:
  image: d2-live-analyzer:latest
  environment:
    - DETOURS_MODE=native_hooks
    - D2_VERSION=configurable
    - CORRELATION_DB_HOST=bsim-postgres
  volumes:
    - ./binaries:/d2-binaries:ro
    - ./analysis-output:/analysis-data
```

### **Static/Live Correlation**
- **Real-time Hooks**: Native Windows Detours for function entry/exit
- **Memory Analysis**: Runtime data structure validation
- **Execution Flow**: Dynamic call graph generation
- **Parameter Validation**: Live parameter value analysis

## ğŸ“Š **Monitoring & Observability**

### **Real-Time Metrics Dashboard**
```
ğŸ“Š System Health Monitoring
â”œâ”€â”€ ğŸ¤– AI Model Performance
â”‚   â”œâ”€â”€ Response times (target: <2s for complex queries)
â”‚   â”œâ”€â”€ Error rates (target: <1% for routine operations)
â”‚   â””â”€â”€ Cost tracking (budget: $50/day with alerts at 80%)
â”œâ”€â”€ ğŸ” Vector Search Performance
â”‚   â”œâ”€â”€ Query response times (target: <500ms)
â”‚   â”œâ”€â”€ Relevance scores (target: >0.7 average)
â”‚   â””â”€â”€ Cache hit rates (target: >60% for repeated queries)
â”œâ”€â”€ ğŸ’¾ Database Performance
â”‚   â”œâ”€â”€ Connection pooling efficiency
â”‚   â”œâ”€â”€ Query optimization metrics
â”‚   â””â”€â”€ Storage growth tracking
â””â”€â”€ ğŸ•·ï¸ Community Mining Health
    â”œâ”€â”€ Discovery rates (functions/day)
    â”œâ”€â”€ Source quality distribution
    â””â”€â”€ Validation success rates
```

### **Automated Alerting**
- **Critical**: System downtime, security breaches (immediate)
- **High**: Performance degradation >50%, budget at 100% (15min)
- **Medium**: Storage at 80%, quality degradation (hourly)
- **Low**: Weekly summaries, optimization opportunities (daily)

## ğŸ”’ **Security Framework**

### **Input Validation & Sanitization**
```python
class SecurityValidator:
    def validate_chat_input(self, user_input, session):
        # HTML/script injection prevention
        # Rate limiting (10 queries/minute per session)
        # Content policy enforcement
        # Session validity checks
        # Query complexity limits

    def validate_community_data(self, source_data):
        # Source authenticity verification
        # Malicious code detection
        # License compatibility checking
        # Privacy compliance validation
        # Technical accuracy cross-validation
```

### **API Security**
- **Authentication**: Session-based for admin features
- **Rate Limiting**: Per-IP and per-session limits
- **HTTPS Enforcement**: All external communications encrypted
- **Input Sanitization**: All user inputs validated and sanitized
- **Audit Logging**: All administrative actions logged

## ğŸ’° **Cost Management**

### **Budget Controls**
- **Daily Budget**: $50 with alerts at 80% consumption
- **Cost Protection**: Automatic model degradation at 100% budget
- **Predictive Alerting**: Trend analysis for usage forecasting
- **Model Optimization**: Continuous evaluation of cost vs quality

### **Expected Cost Distribution**
```
Monthly Cost Breakdown (Estimated):
â”œâ”€â”€ AI Model Usage: ~$1,200/month (80% of budget)
â”‚   â”œâ”€â”€ Opus 4.6 (20% usage): ~$600/month
â”‚   â”œâ”€â”€ Sonnet (30% usage): ~$450/month
â”‚   â””â”€â”€ Haiku (50% usage): ~$150/month
â”œâ”€â”€ Infrastructure: ~$100/month (PostgreSQL, containers)
â”œâ”€â”€ External APIs: ~$50/month (GitHub API, other sources)
â””â”€â”€ Storage & Bandwidth: ~$50/month
Total: ~$1,400/month (vs ~$4,200 for all-Opus approach)
```

## ğŸš€ **Deployment Architecture**

### **Container Orchestration**
```yaml
services:
  # Existing services (preserved)
  bsim-postgres:    # Enhanced with pgvector
  ghidra-server:    # Existing Ghidra backend
  ghidra-web:       # Django web interface

  # New AI services
  ai-orchestrator:  # Multi-model coordination
  vector-search:    # Semantic search engine
  community-miner:  # Knowledge discovery
  live-analyzer:    # Windows analysis container

  # Enhanced monitoring
  monitoring-dashboard: # System health and metrics
  alert-manager:    # Notification and alerting
```

### **Data Flow**
```
User Query â†’ Chat Interface â†’ AI Orchestrator â†’ Task Classification
    â†“
[Sonnet Analysis] OR [Haiku Processing] OR [Opus Coordination]
    â†“
Vector Search â† Knowledge Integration â†’ Community Mining
    â†“
Response Generation â† BSim Database â† Live Analysis
    â†“
User Response + Feedback Collection â†’ Performance Optimization
```

## ğŸ“ˆ **Scalability & Performance**

### **Horizontal Scaling Strategy**
- **AI Orchestration**: Stateless services with load balancing
- **Vector Search**: Read replicas for search-heavy workloads
- **Database**: PostgreSQL streaming replication
- **Caching**: Redis for frequently accessed data

### **Performance Targets**
- **Chat Response Time**: <2 seconds for 95% of queries
- **Vector Search**: <500ms for semantic queries
- **Database Queries**: <100ms for simple lookups
- **Page Load Times**: <1 second for function detail pages
- **System Uptime**: >99.9% availability

## ğŸ”„ **Data Lifecycle Management**

### **Knowledge Evolution**
```
Discovery â†’ Validation â†’ Integration â†’ Verification â†’ Optimization
     â†“         â†“           â†“             â†“              â†“
Community  Trust Score  Vector Store  Binary Check  Performance
Sources    Calculation  Integration   Validation    Tuning
```

### **Quality Assurance**
- **Continuous Validation**: Community knowledge vs binary analysis
- **Trust Score Updates**: Source reliability tracking over time
- **Performance Monitoring**: Query quality and user satisfaction
- **Automated Cleanup**: Low-quality data removal after review period

---

## ğŸ“‹ **Implementation Phases**

### **Phase 1: Foundation (Days 1-7)**
Core infrastructure, database enhancement, basic AI orchestration

### **Phase 2: Advanced Features (Days 8-16)**
Enhanced chat, community mining, live analysis integration

### **Phase 3: Optimization (Days 17-25)**
Performance tuning, advanced analytics, production readiness

---

This master architecture provides a comprehensive foundation for building an intelligent, cost-effective, and scalable reverse engineering platform that preserves the proven workflows while adding transformative AI capabilities.